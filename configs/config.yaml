app:
  title: "Bacterial GAN Augmentation API"
  version: "0.1.0"

data:
  raw_data_dir: "data/01_raw"
  processed_data_dir: "data/02_processed"
  synthetic_data_dir: "data/03_synthetic"
  expert_testing_set_path: "data/04_expert_testing"

preprocessing:
  # Patch extraction settings (NEW: extracts patches instead of resizing)
  use_patch_extraction: true # If true, extracts all possible patches. If false, extracts a single random patch per image.
  image_size: 256  # Patch size to extract from high-res images (256x256)
  apply_augmentation: false  # Apply traditional augmentation (rotations + flips, 8x multiplier)
  bg_threshold: 0.9  # Background filtering threshold (0.9 = discard patches with >90% white space)
  max_patches_per_split: 5000 # Limit dataset size to prevent storage explosion

  # Data splitting
  train_ratio: 0.7  # Training set ratio (70%)
  val_ratio: 0.15  # Validation set ratio (15%)
  test_ratio: 0.15  # Test set ratio (15%)
  random_seed: 42  # Random seed for reproducible splits

  # Macenko normalization parameters (NOT recommended for this project)
  apply_macenko_normalization: false  # Enable Macenko to normalize background lighting variations
  macenko_io: 240  # Background light intensity
  macenko_alpha: 1.0  # Percentile for angle calculation
  macenko_beta: 0.15  # Optical density threshold for background filtering

training:
  # Model architecture
  image_size: 256  # Input/output image size (must match preprocessing.image_size)
  num_classes: 2  # Number of bacterial classes (gram-positive, gram-negative)
  channels: 3  # RGB channels
  latent_dim: 100  # Dimension of noise vector for generator

  # Training hyperparameters
  batch_size: 16  # Batch size for training (reduced to 16 to prevent OOM with high-capacity model)
  epochs: 300  # Total number of training epochs
  
  # Two-Timescale Update Rule (TTUR) - separate learning rates for G and D
  # Generator learns faster to prevent critic drift
  learning_rate_g: 0.0001  # Generator learning rate (4x faster than D)
  learning_rate_d: 0.0002  # Discriminator learning rate (slower to prevent domination)
  learning_rate: 0.0001  # Fallback for backward compatibility (deprecated)
  
  beta1: 0.0  # Adam optimizer beta1 parameter (0.0 for WGAN-GP)
  beta2: 0.9  # Adam optimizer beta2 parameter (0.9 for WGAN-GP)

  # GAN loss settings
  loss_type: "wgan-gp"  # Loss function: "wgan-gp", "lsgan", or "vanilla"
  n_critic: 3  # Reduced from 5 to prevent critic drift (modern WGAN-GP practice)
  lambda_gp: 10.0  # Gradient penalty coefficient (for WGAN-GP)

  # Performance settings
  use_mixed_precision: false  # Use mixed precision training (faster on modern GPUs)

  # Memory optimization settings
  memory_optimization:
    # GPU memory configuration
    gpu_memory_growth: true  # Enable dynamic GPU memory allocation (recommended for 4GB VRAM)
    gpu_memory_limit_mb: null  # Hard GPU memory limit in MB (null = use growth instead, e.g., 3072 for 3GB limit)

    # CPU parallelization
    cpu_threads: 12  # Number of CPU threads for data loading (null = auto-detect)

    # XLA compilation
    enable_xla: false  # Enable XLA compilation (⚠️ DISABLE for 4GB VRAM to prevent OOM, enable for >8GB VRAM)

    # Dataset pipeline optimization
    dataset_prefetch_buffer: -1  # Prefetch buffer size (-1 = AUTOTUNE, or set to specific value like 2)
    dataset_cache_in_memory: false  # Cache dataset in RAM (only for small datasets <1GB)
    dataset_cache_filename: null  # Disk-based cache file path (null = no disk caching, e.g., "data/cache/train.tfcache")

  # Output and checkpointing
  checkpoint_interval: 50  # Save model checkpoint every N epochs
  sample_interval: 10  # Generate sample images every N epochs
  num_samples_during_training: 4  # Number of sample images to generate during training
  num_samples_final: 8  # Number of sample images to generate at end of training
  num_samples_grid: 16  # Total samples for visualization grid (must be perfect square)

  # Output directories
  models_output_dir: "models"  # Base directory for saving model checkpoints
  samples_output_dir: "samples"  # Base directory for saving sample images

  # MLflow settings
  mlflow_experiment_name: "Bacterial GAN Augmentation"  # MLflow experiment name
  mlflow_tracking_uri: ""  # MLflow tracking URI (empty = local ./mlruns)

  # Dummy dataset settings (for testing when no real data available)
  dummy_num_batches: 10  # Number of batches in dummy dataset

evaluation:
  num_images_for_expert_panel: 25
